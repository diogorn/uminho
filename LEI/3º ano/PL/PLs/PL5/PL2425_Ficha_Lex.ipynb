{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4GdHi2G0OlbE"
      },
      "source": [
        "# Ficha de Análise Léxica (Lex)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0JdE83EOlb0"
      },
      "source": [
        "## Introdução"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mC00B5-IOlb1"
      },
      "source": [
        "A análise léxica é o processo de conversão de uma sequência de caracteres numa sequência de *tokens*, em que cada *token* representa uma unidade significativa da linguagem à qual os caracteres pertencem.\n",
        "\n",
        "Em Python, podemos fazer análise léxica de várias formas. A que iremos utilizar nas aulas recorre ao módulo Ply, que para além de análise léxica vai-nos permitir fazer análise sintática.\n",
        "\n",
        "Antes de usar o módulo Ply, precisamos de o instalar. Para isso, podemos usar o comando seguinte:\n",
        "\n",
        "```sh\n",
        "$ pip install ply\n",
        "```\n",
        "\n",
        "Depois, apenas precisamos de importar a ferramenta `lex.py` no nosso programa:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "5yXKYfAnOlcJ"
      },
      "outputs": [],
      "source": [
        "import ply.lex as lex"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Lx_dqWwkOlcL"
      },
      "source": [
        "A primeira coisa que o nosso analisador léxico (ou *lexer*/*tokenizer*) precisa de ter é uma lista de *tokens*. Como exemplo, vamos definir um *lexer* que lê expressões aritméticas, como \"4 * (2 + 3)\". Neste exemplo já somos capazes de identificar alguns *tokens*..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XgXrbPicOlcM"
      },
      "outputs": [],
      "source": [
        "tokens = (\n",
        "    'NUMBER',\n",
        "    'PLUS',\n",
        "    # completar...\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qsNXH-ZeOlcM"
      },
      "source": [
        "A seguir é preciso especificar cada *token*. Por outras palavras, precisamos de definir expressões regulares que permitam ao *tokenizer* identificar os *tokens*. Podemos fazê-lo através de variáveis ou de funções."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F95mz8RYOlcb"
      },
      "outputs": [],
      "source": [
        "t_PLUS = r'\\+'\n",
        "# completar...\n",
        "\n",
        "def t_NUMBER(t):\n",
        "  r'\\d+'\n",
        "  t.value = int(t.value)\n",
        "  return t\n",
        "\n",
        "    # completar"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ow2ILFtkOlcc"
      },
      "source": [
        "Podemos especificar um conjunto de caracteres que o analisador léxico vai ignorar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5fxalLuTOlcd"
      },
      "outputs": [],
      "source": [
        "t_ignore = ' \\t\\n'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3b9xt-tqOlce"
      },
      "source": [
        "Precisamos ainda de definir o comportamento do *tokenizer* caso encontre um carácter ou sequência de caracteres que não corresponda a nenhum *token* conhecido."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XYqjFDTEOlcp"
      },
      "outputs": [],
      "source": [
        "def t_error(t):\n",
        "    print(f\"Carácter ilegal {t.value[0]}\")\n",
        "    t.lexer.skip(1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5hMOGfxsOldI"
      },
      "source": [
        "Agora, já somos capazes de construir o nosso analisador léxico."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DN9vAe7UOldP"
      },
      "outputs": [],
      "source": [
        "lexer = lex.lex()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjACXglAOldP"
      },
      "source": [
        "Para o usar, precisamos de lhe dar algum valor de *input* e depois pedir-lhe para ir devolvendo os *tokens* que encontrar."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f5BEZj6XOldQ"
      },
      "outputs": [],
      "source": [
        "data = '''\n",
        "3 + 4 * 10\n",
        "  + -20 *2\n",
        "'''\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "while tok := lexer.token():\n",
        "    print(tok)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Uvix55yOldz"
      },
      "source": [
        "Se quisermos manter informação sobre as linhas nas quais os *tokens* aparecem, podemos usar o atributo `lineno`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n5CFURcLOleU"
      },
      "outputs": [],
      "source": [
        "t_ignore = ' \\t'\n",
        "\n",
        "def t_newline(t):\n",
        "    r'\\n+'\n",
        "    t.lexer.lineno += len(t.value)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPrQMVnjOldQ"
      },
      "source": [
        "É possível consultar a documentação do *lex.py* em https://ply.readthedocs.io/en/latest/ply.html#lex."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iyge6OSnOldR"
      },
      "source": [
        "## Exercícios"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s3hIQxn8OldR"
      },
      "source": [
        "### 1. Frases\n",
        "\n",
        "Define um analisador léxico capaz de ler uma frase e de identificar os seus componentes (palavras, vírgulas, sinais de pontuação).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: /var/folders/xx/xzf1j1cd4rz4spn6h9d0gjhh0000gn/T/ipykernel_97821/2306068294.py:35: Invalid regular expression for rule 't_ID'. bad character range a-Z at position 10\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "<module '__main__'> is a built-in module",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 53\u001b[39m\n\u001b[32m     50\u001b[39m     t.lexer.skip(\u001b[32m1\u001b[39m)\n\u001b[32m     52\u001b[39m \u001b[38;5;66;03m#build the lexer\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m lexer = \u001b[43mlex\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     55\u001b[39m lexer.input(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m tok \u001b[38;5;129;01min\u001b[39;00m lexer:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidade/Arquivo/LEI/3º ano/2º Semestre/PL/PLs/Sem5/myenv/lib/python3.13/site-packages/ply/lex.py:908\u001b[39m, in \u001b[36mlex\u001b[39m\u001b[34m(module, object, debug, optimize, lextab, reflags, nowarn, outputdir, debuglog, errorlog)\u001b[39m\n\u001b[32m    906\u001b[39m linfo.get_all()\n\u001b[32m    907\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m optimize:\n\u001b[32m--> \u001b[39m\u001b[32m908\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlinfo\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalidate_all\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    909\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mSyntaxError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mCan\u001b[39m\u001b[33m'\u001b[39m\u001b[33mt build lexer\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    911\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m optimize \u001b[38;5;129;01mand\u001b[39;00m lextab:\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidade/Arquivo/LEI/3º ano/2º Semestre/PL/PLs/Sem5/myenv/lib/python3.13/site-packages/ply/lex.py:579\u001b[39m, in \u001b[36mLexerReflect.validate_all\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m \u001b[38;5;28mself\u001b[39m.validate_tokens()\n\u001b[32m    578\u001b[39m \u001b[38;5;28mself\u001b[39m.validate_literals()\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_rules\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.error\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidade/Arquivo/LEI/3º ano/2º Semestre/PL/PLs/Sem5/myenv/lib/python3.13/site-packages/ply/lex.py:821\u001b[39m, in \u001b[36mLexerReflect.validate_rules\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    818\u001b[39m             \u001b[38;5;28mself\u001b[39m.error = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    820\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.modules:\n\u001b[32m--> \u001b[39m\u001b[32m821\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvalidate_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/Universidade/Arquivo/LEI/3º ano/2º Semestre/PL/PLs/Sem5/myenv/lib/python3.13/site-packages/ply/lex.py:833\u001b[39m, in \u001b[36mLexerReflect.validate_module\u001b[39m\u001b[34m(self, module)\u001b[39m\n\u001b[32m    831\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mvalidate_module\u001b[39m(\u001b[38;5;28mself\u001b[39m, module):\n\u001b[32m    832\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m833\u001b[39m         lines, linen = \u001b[43minspect\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgetsourcelines\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    834\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mIOError\u001b[39;00m:\n\u001b[32m    835\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/inspect.py:1238\u001b[39m, in \u001b[36mgetsourcelines\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m   1230\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"Return a list of source lines and starting line number for an object.\u001b[39;00m\n\u001b[32m   1231\u001b[39m \n\u001b[32m   1232\u001b[39m \u001b[33;03mThe argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1235\u001b[39m \u001b[33;03moriginal source file the first line of code was found.  An OSError is\u001b[39;00m\n\u001b[32m   1236\u001b[39m \u001b[33;03mraised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[32m   1237\u001b[39m \u001b[38;5;28mobject\u001b[39m = unwrap(\u001b[38;5;28mobject\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1238\u001b[39m lines, lnum = \u001b[43mfindsource\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m istraceback(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m   1241\u001b[39m     \u001b[38;5;28mobject\u001b[39m = \u001b[38;5;28mobject\u001b[39m.tb_frame\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/inspect.py:1060\u001b[39m, in \u001b[36mfindsource\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m   1052\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfindsource\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m   1053\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the entire source file and starting line number for an object.\u001b[39;00m\n\u001b[32m   1054\u001b[39m \n\u001b[32m   1055\u001b[39m \u001b[33;03m    The argument may be a module, class, method, function, traceback, frame,\u001b[39;00m\n\u001b[32m   1056\u001b[39m \u001b[33;03m    or code object.  The source code is returned as a list of all the lines\u001b[39;00m\n\u001b[32m   1057\u001b[39m \u001b[33;03m    in the file and the line number indexes a line in that list.  An OSError\u001b[39;00m\n\u001b[32m   1058\u001b[39m \u001b[33;03m    is raised if the source code cannot be retrieved.\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1060\u001b[39m     file = \u001b[43mgetsourcefile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1061\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m file:\n\u001b[32m   1062\u001b[39m         \u001b[38;5;66;03m# Invalidate cache if needed.\u001b[39;00m\n\u001b[32m   1063\u001b[39m         linecache.checkcache(file)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/inspect.py:963\u001b[39m, in \u001b[36mgetsourcefile\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m    959\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mgetsourcefile\u001b[39m(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m    960\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the filename that can be used to locate an object's source.\u001b[39;00m\n\u001b[32m    961\u001b[39m \u001b[33;03m    Return None if no way can be identified to get the source.\u001b[39;00m\n\u001b[32m    962\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m963\u001b[39m     filename = \u001b[43mgetfile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mobject\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    964\u001b[39m     all_bytecode_suffixes = importlib.machinery.DEBUG_BYTECODE_SUFFIXES[:]\n\u001b[32m    965\u001b[39m     all_bytecode_suffixes += importlib.machinery.OPTIMIZED_BYTECODE_SUFFIXES[:]\n",
            "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/Cellar/python@3.13/3.13.2/Frameworks/Python.framework/Versions/3.13/lib/python3.13/inspect.py:924\u001b[39m, in \u001b[36mgetfile\u001b[39m\u001b[34m(object)\u001b[39m\n\u001b[32m    922\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mobject\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m__file__\u001b[39m\u001b[33m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m    923\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mobject\u001b[39m.\u001b[34m__file__\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m924\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[33m'\u001b[39m\u001b[38;5;132;01m{!r}\u001b[39;00m\u001b[33m is a built-in module\u001b[39m\u001b[33m'\u001b[39m.format(\u001b[38;5;28mobject\u001b[39m))\n\u001b[32m    925\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m isclass(\u001b[38;5;28mobject\u001b[39m):\n\u001b[32m    926\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mobject\u001b[39m, \u001b[33m'\u001b[39m\u001b[33m__module__\u001b[39m\u001b[33m'\u001b[39m):\n",
            "\u001b[31mTypeError\u001b[39m: <module '__main__'> is a built-in module"
          ]
        }
      ],
      "source": [
        "import ply.lex as lex\n",
        "\n",
        "input = '''\n",
        "? a\n",
        "? b \n",
        "c = a*b/2\n",
        "d = f(c,2)\n",
        "'''\n",
        "\n",
        "tokens = (\n",
        "    'NUM',\n",
        "    'MUL',\n",
        "    'DIV',\n",
        "    'PA',\n",
        "    'PF',\n",
        "    'READ',\n",
        "    'ATRIB',\n",
        "    'VIRG',\n",
        "    'ID')\n",
        "\n",
        "# regular expressions rules for a simple tokens\n",
        "t_READ = r'\\?'\n",
        "t_ATRIB = r'='\n",
        "t_MUL = r'\\*'\n",
        "t_DIV = r'\\/'\n",
        "t_PA = r'\\('\n",
        "t_PF = r'\\)'\n",
        "t_VIRG = r','\n",
        "\n",
        "def t_NUM(t):\n",
        "    r'\\d+'\n",
        "    t.value = int(t.value)\n",
        "    return t\n",
        "\n",
        "def t_ID(t):\n",
        "    r'[a-ZA-Z_]\\w*'\n",
        "    return t\n",
        "\n",
        "def t_newline(t):\n",
        "    r'\\n+'\n",
        "    t.lexer.lineno += len(t.value)\n",
        "\n",
        "\n",
        "# A string containing ignored characters (spaces and tabs)\n",
        "t_ignore = ' \\t'\n",
        "\n",
        "# error handling rule\n",
        "def t_error(t):\n",
        "    print(\"Illegal character {t.value[0]}\")\n",
        "    t.lexer.skip(1)\n",
        "\n",
        "#build the lexer\n",
        "lexer = lex.lex()\n",
        "\n",
        "lexer.input(input)\n",
        "for tok in lexer:\n",
        "    print(tok)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UST_08V9OldS"
      },
      "source": [
        "### 2. Listas Mistas\n",
        "\n",
        "Define um analisador léxico capaz de receber listas com números, palavras ou valores booleanos como input (e.g.: `[ 1,5, palavra, False ,3.14,   0, fim]`) e identificar os seus *tokens*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7s__Lct3Oldh"
      },
      "source": [
        "### 3. JSON\n",
        "\n",
        "Define um analisador léxico capaz de ler ficheiros em formato JSON e identificar os seus *tokens*.\n",
        "\n",
        "Exemplo de um documento JSON:\n",
        "\n",
        "---\n",
        "\n",
        "```json\n",
        "{\n",
        "  \"name\": \"John Doe\",\n",
        "  \"age\": 21,\n",
        "  \"gender\": \"male\",\n",
        "  \"height\": 1.68,\n",
        "  \"address\": {\n",
        "    \"street\": \"123 Main Street\",\n",
        "    \"city\": \"New York\",\n",
        "    \"country\": \"USA\",\n",
        "    \"zip\": \"10001\"\n",
        "  },\n",
        "  \"married\": false,\n",
        "  \"hobbies\": [\n",
        "    {\n",
        "      \"name\": \"reading\",\n",
        "      \"books\": [\n",
        "        {\n",
        "          \"title\": \"Heartstopper: Volume 1\",\n",
        "          \"author\": \"Alice Oseman\",\n",
        "          \"genres\": [\"Graphic Novels\", \"Romance\", \"Queer\"]\n",
        "        },\n",
        "        {\n",
        "          \"title\": \"1984\",\n",
        "          \"author\": \"George Orwell\",\n",
        "          \"genres\": [\"Science Fiction\", \"Dystopia\", \"Politics\"]\n",
        "        }\n",
        "      ]\n",
        "    },\n",
        "    {\n",
        "      \"name\": \"gaming\",\n",
        "      \"games\": [\n",
        "        {\n",
        "          \"title\": \"Portal 2\",\n",
        "          \"platform\": [\"PC\", \"PlayStation 3\", \"Xbox 360\"]\n",
        "        },\n",
        "        {\n",
        "          \"title\": \"Synth Riders\",\n",
        "          \"platform\": [\"PSVR\", \"PSVR2\", \"PCVR\", \"Oculus Quest\"]\n",
        "        }\n",
        "      ]\n",
        "    }\n",
        "  ]\n",
        "}\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xTLcLtKAOldi"
      },
      "source": [
        "## Condições de contexto\n",
        "\n",
        "Para certos analisadores léxico, pode ser útil ter diferentes estados. Por exemplo, se definirmos um analisador léxico para um ficheiro XML, pode ser útil verificar se o nome usado para fechar uma *tag* foi o mesmo que foi usado para a abrir.\n",
        "\n",
        "Exemplo de parte de um ficheiro XML:\n",
        "\n",
        "```xml\n",
        "<pessoa>\n",
        "    <nome>Maria</nome>\n",
        "    <idade>32</idade>\n",
        "</pessoa>\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QDH2v3NjOldw"
      },
      "outputs": [],
      "source": [
        "import ply.lex as lex\n",
        "\n",
        "states = (\n",
        "    ('taga', 'exclusive'),\n",
        "    ('tagf', 'exclusive'), # num estado exclusivo, apenas aplicamos os tokens e regras para esse estado\n",
        "                           # por outro lado, num estado inclusivo, as regras e tokens desse estado juntam-se às outras regras e tokens\n",
        "                           # o estado inicial chama-se 'INITIAL' e não é preciso defini-lo\n",
        ")\n",
        "\n",
        "tokens = (\n",
        "    'ABRIR_TAG',      # '<'\n",
        "    'ABRIR_TAG_F',    # '</'\n",
        "    'FECHAR_TAG',     # '>'\n",
        "    'NOME_TAG',       # Palavra case-insensitive\n",
        "    'VALOR'           # quaisquer Carateres entre Tags\n",
        ")\n",
        "\n",
        "t_ignore = ' \\t\\n' # estes tokens apenas são ignorados no estado 'INITIAL' e em estados inclusivos\n",
        "\n",
        "t_VALOR = r'[^<]+'\n",
        "\n",
        "def t_ABRIR_TAG_F(t):\n",
        "    r'</'\n",
        "    t.lexer.begin('tagf') # entramos no estado 'tagf'\n",
        "    return t\n",
        "\n",
        "def t_ABRIR_TAG(t):\n",
        "    r'<'\n",
        "    t.lexer.begin('taga') # entramos no estado 'taga'\n",
        "    return t\n",
        "\n",
        "def t_taga_tagf_FECHAR_TAG(t):\n",
        "    r'>'\n",
        "    t.lexer.begin('INITIAL') # voltamos ao estado inicial\n",
        "    return t\n",
        "\n",
        "def t_taga_NOME_TAG(t):\n",
        "    r'\\w+'\n",
        "    t.lexer.stack.append(t.value)\n",
        "    return t\n",
        "\n",
        "def t_tagf_NOME_TAG(t):\n",
        "    r'\\w+'\n",
        "    if len(t.lexer.stack) > 0:\n",
        "        if (nt := t.lexer.stack.pop(-1)) != t.value:\n",
        "            print(f\"Erro - esperado nome de tag '{nt}', mas foi lido '{t.value}'!\")\n",
        "    else:\n",
        "        print(\"Erro - nenhuma tag aberta!\")\n",
        "    return t\n",
        "\n",
        "def t_ANY_error(t): # regra válida para todos os estados\n",
        "    print(f\"Carácter ilegal: {t.value[0]}\")\n",
        "    t.lexer.skip(1)\n",
        "\n",
        "\n",
        "data = '''\n",
        "<pessoa>\n",
        "    <nome>Maria</nome>\n",
        "    <idade>32</idade>\n",
        "</pessoa>\n",
        "'''\n",
        "\n",
        "lexer = lex.lex()\n",
        "\n",
        "lexer.stack = list() # vamos usar esta lista como stack para verificar os nomes das tags\n",
        "\n",
        "lexer.input(data)\n",
        "\n",
        "while tok := lexer.token():\n",
        "    print(tok)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJqjWeSfOleV"
      },
      "source": [
        "## Exercícios 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onMLXP8gOleY"
      },
      "source": [
        "### 1. BibTeX\n",
        "\n",
        "Define um analisador léxico capaz de ler um ficheiro no formato *BibTeX* e identificar os seus *tokens*.\n",
        "\n",
        "Exemplo de um ficheiro BibTeX:\n",
        "\n",
        "---\n",
        "\n",
        "```bibtex\n",
        "@incollection {HDYE78,\n",
        "author = \"Ricardo Martini and Pedro Rangel Henriques and Giovani Libreloto\",\n",
        "title = \"Storing Archival Emigration Documents to Create Virtual Exhibition Rooms\",\n",
        "booktitle = \"New Contributions in Information Systems and Technologies\",\n",
        "series=\"Advances in Intelligent Systems and Computing\",\n",
        "editor=\"Rocha, Alvaro and Correia, Ana and Costanzo, S. and Reis, Luis Paulo\",\n",
        "volume=\"353\",\n",
        "pages=\"403-409\",\n",
        "year = \"2015\",\n",
        "month =  \"April\"\n",
        "}\n",
        "\n",
        "\n",
        "@book {H787,\n",
        "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
        "title = {An AST-based tool, Spector, for Plagiarism Detection},\n",
        "booktitle = {Proceedings of SLATE’15},\n",
        "pages = {173--178},\n",
        "ISBN = {},\n",
        "year = {2015},\n",
        "month =   {},\n",
        "publisher = {Fundacion General UCM},\n",
        "annote = {Keywords: software, plagiarism, detection, comparison, test}}\n",
        "\n",
        "@book {H787,\n",
        "author = {Vitor T. Martins and Pedro Rangel Henriques and Daniela da Cruz},\n",
        "title = \"{A}n {AST}-based tool, {S}pector, for Plagiarism Detection\",\n",
        "booktitle = {Proceedings of SLATE’15},\n",
        "pages = {173--178},\n",
        "ISBN = {},\n",
        "year = {2015},\n",
        "month =   {},\n",
        "publisher = {Fundaci ́on General UCM},\n",
        "annote = {Keywords: software, plagiarism, detection, comparison, test}\n",
        "}\n",
        "```\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VxXn9uTSOlep"
      },
      "source": [
        "### 2. Somador on/off\n",
        "\n",
        "Usando um analisador léxico com condições de contexto, cria um programa em Python que tenha o seguinte comportamento:\n",
        "\n",
        "* Pretende-se um programa que some todas as sequências de dígitos que encontre num texto;\n",
        "* Prepara o programa para ler o texto do canal de entrada: stdin;\n",
        "* Sempre que encontrar a string “Off” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é desligado;\n",
        "* Sempre que encontrar a string “On” em qualquer combinação de maiúsculas e minúsculas, esse comportamento é novamente ligado;\n",
        "* Sempre que encontrar o caráter “=”, o resultado da soma é colocado na saída."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQePKCJEg8Oj"
      },
      "source": [
        "### 3. Removedor de Comentários\n",
        "\n",
        "Desenvolve um analisador léxico capaz de ler um ficheiro de texto e ignorar todo o texto dentro de comentários inline (desde \"//\" até ao fim de linha) e todo o texto dentro de comentários multiline (desde \"/*\" até \"*/\").\n",
        "\n",
        "O *lexer* deve suportar convenientemente comentários dentro de comentários,   conforme exemplificado abaixo:\n",
        "\n",
        "---\n",
        "```c\n",
        "/* comment */ ola1\n",
        "\n",
        "/* comment****comment */ ola2 /*\n",
        "comment\n",
        "/* comentário dentro de comentário */\n",
        "****/ ola3\n",
        "\n",
        "/*********/\n",
        "\n",
        "ola4\n",
        " mais um pouco // remover comentário inline\n",
        "FIM\n",
        "```\n",
        "----\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "myenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.2"
    },
    "orig_nbformat": 4
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
